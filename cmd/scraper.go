package cmd

import (
	"context"
	"fmt"
	"log"
	"time"

	"web-text-pipe-go/pkg/scraperfactory" // ğŸ’¡ factory ã‹ã‚‰ scraperfactory ã«å¤‰æ›´
	"web-text-pipe-go/pkg/scraperrunner"

	"github.com/shouni/go-cli-base"
	"github.com/shouni/go-web-exact/v2/pkg/types"
	"github.com/spf13/cobra"
)

// --- ãƒ­ã‚¸ãƒƒã‚¯: çµæœã®å‡ºåŠ› (I/O) ---

// printResults ã¯ã€scraperrunnerã‹ã‚‰å—ã‘å–ã£ãŸçµæœã‚’CLIã«å‡ºåŠ›ã—ã¾ã™ã€‚
func printResults(results []types.URLResult, verbose bool) {
	fmt.Println("\n--- ä¸¦åˆ—ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ ---")
	successCount := 0
	errorCount := 0

	for i, res := range results {
		if res.Error != nil {
			errorCount++
			log.Printf("âŒ [%d] %s\n     ã‚¨ãƒ©ãƒ¼: %v\n", i+1, res.URL, res.Error)
		} else {
			successCount++
			if verbose {
				fmt.Printf("âœ… [%d] %s\n     æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•: %d æ–‡å­—\n     ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: %s...\n",
					i+1, res.URL, len(res.Content), res.Content[:min(len(res.Content), 50)])
			} else {
				fmt.Printf("âœ… [%d] %s\n     æŠ½å‡ºã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•: %d æ–‡å­—\n", i+1, res.URL, len(res.Content))
			}
		}
	}

	fmt.Println("-------------------------------")
	log.Printf("å®Œäº†: æˆåŠŸ %d ä»¶, å¤±æ•— %d ä»¶\n", successCount, errorCount)
}

// --- ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰å®šç¾© ---

var scraperCmd = &cobra.Command{
	Use:   "scraper",
	Short: "RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰URLã‚’æŠ½å‡ºã—ã€Webã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¸¦åˆ—ã§å–å¾—ãƒ»æ•´å½¢ã—ã¾ã™",
	Long: `--url ãƒ•ãƒ©ã‚°ã§æŒ‡å®šã•ã‚ŒãŸRSS/Atomãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è§£æã—ã€å«ã¾ã‚Œã‚‹è¨˜äº‹ã®URLã‚’æŠ½å‡ºã—ã€
æŒ‡å®šã•ã‚ŒãŸæœ€å¤§åŒæ™‚å®Ÿè¡Œæ•°ã§ä¸¦åˆ—ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚’å®Ÿè¡Œã—ã¾ã™ã€‚`,
	Args: cobra.NoArgs,

	RunE: func(cmd *cobra.Command, args []string) error {
		// 1. ãƒ•ãƒ©ã‚°å€¤ã®å–å¾—ã¨è¨­å®šã®æ§‹ç¯‰
		feedURL, _ := cmd.Flags().GetString("url")
		concurrency, _ := cmd.Flags().GetInt("concurrency")
		clientTimeout := time.Duration(Flags.TimeoutSec) * time.Second

		// 2. scraperfactory ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€Runnerã‚’å–å¾—
		runner, err := scraperfactory.BuildScraperRunner(clientTimeout, concurrency)
		if err != nil {
			return err
		}

		// 3. å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨è¨­å®šã®æº–å‚™
		ctx := context.Background()
		config := scraperrunner.RunnerConfig{
			FeedURL:                  feedURL,
			ClientTimeout:            clientTimeout,
			OverallTimeoutMultiplier: 2,
		}

		// 4. ScrapeAndRun ã®å‘¼ã³å‡ºã—
		results, err := runner.ScrapeAndRun(ctx, config)
		if err != nil {
			return err
		}

		// 5. çµæœã®å‡ºåŠ›
		printResults(results, clibase.Flags.Verbose)

		return nil
	},
}

// --- ãƒ•ãƒ©ã‚°åˆæœŸåŒ– ---

func initScraperFlags() {
	scraperCmd.Flags().StringP("url", "u", "https://news.yahoo.co.jp/rss/categories/it.xml", "è§£æå¯¾è±¡ã®ãƒ•ã‚£ãƒ¼ãƒ‰URL (RSS/Atom)")
	scraperCmd.Flags().IntP("concurrency", "c", scraperrunner.DefaultMaxConcurrency, "æœ€å¤§ä¸¦åˆ—å®Ÿè¡Œæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 6)")
}
